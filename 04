import pandas as pd
import math

def base_entropy(dataset):
    p = 0
    n = 0
    target = dataset.iloc[:, -1]
    targets = list(set(target))
    for i in target:
        if i == targets[0]:
            p = p + 1
        else:
            n = n + 1
    if p == 0 or n == 0:
        return 0
    elif p == n:
        return 1
    else:
        entropy = 0 - (
            ((p / (p + n)) * (math.log2(p / (p + n))) + (n / (p + n)) * (math.log2(n/ (p + n)))))
        return entropy

def entropy(dataset, feature, attribute):
    p = 0
    n = 0
    target = dataset.iloc[:, -1]
    targets = list(set(target))
    for i, j in zip(feature, target):
        if i == attribute and j == targets[0]:
            p = p + 1
        elif i == attribute and j == targets[1]:
            n = n + 1
        if p == 0 or n == 0:
            return 0
        elif p == n:
            return 1
        else:
            entropy = 0 - (
                ((p / (p + n)) * (math.log2(p / (p + n))) + (n / (p + n)) * (math.log2(n/ (p + n)))))
            return entropy

def counter(target, attribute, i):
    p = 0
    n = 0
    targets = list(set(target))
    for j, k in zip(target, attribute):
        if j == targets[0] and k == i:
            p = p + 1
        elif j == targets[1] and k == i:
            n = n + 1
    return p, n

def Information_Gain(dataset, feature):
    Distinct = list(set(feature))
    Info_Gain = 0
    for i in Distinct:
        Info_Gain = Info_Gain + feature.count(i) / len(feature) * entropy(dataset,feature, i)
        Info_Gain = base_entropy(dataset) - Info_Gain
    return Info_Gain

def generate_childs(dataset, attribute_index):
    distinct = list(dataset.iloc[:, attribute_index])
    childs = dict()
    for i in distinct:
        childs[i] = counter(dataset.iloc[:, -1], dataset.iloc[:, attribute_index], i)
    return childs

def modify_data_set(dataset,index, feature, impurity):
    size = len(dataset)
    subdata = dataset[dataset[feature] == impurity]
    del (subdata[subdata.columns[index]])
    return subdata

def greatest_information_gain(dataset):
    max = -1
    attribute_index = 0
    size = len(dataset.columns) - 1
    for i in range(0, size):
        feature = list(dataset.iloc[:, i])
        i_g = Information_Gain(dataset, feature)
        if max < i_g:
            max = i_g
            attribute_index = i
    return attribute_index

def construct_tree(dataset, tree):
    impure_childs = []
    attribute_index = greatest_information_gain(dataset)
    childs = generate_childs(dataset, attribute_index)
    tree[dataset.columns[attribute_index]] = childs
    targets = list(set(dataset.iloc[:, -1]))
    for k, v in childs.items():
        if v[0] == 0:
            tree[k] = targets[1]
        elif v[1] == 0:
            tree[k] = targets[0]
        elif v[0] != 0 or v[1] != 0:
            impure_childs.append(k)
    for i in impure_childs:
        sub = modify_data_set(dataset,attribute_index,
        dataset.columns[attribute_index], i)
        tree = construct_tree(sub, tree)
    return tree

def main():
    df = pd.read_csv("playtennis.csv")
    tree = dict()
    result = construct_tree(df, tree)
    for key, value in result.items():
        print(key, " => ", value)

if __name__ == "__main__":
    main()





outlook	temperature	humidity	wind	answer
sunny	hot	high	weak	no
sunny	hot	high	strong	no
overcast	hot	high	weak	yes
rain	mild	high	weak	yes
rain	cool	normal	weak	yes
rain	cool	normal	strong	no
overcast	cool	normal	strong	yes
sunny	mild	high	weak	no
sunny	cool	normal	weak	yes
rain	mild	normal	weak	yes
sunny	mild	normal	strong	yes
overcast	mild	high	strong	yes
overcast	hot	normal	weak	yes
rain	mild	high	strong	no







Lab Manual's
import pandas as pd
import math

class Node:
    def __init__(self, l):
        self.label = l
        self.branch = {}

def decision_tree(data):
    root = Node("NULL")
    if(entropy(data)==0):
        if(len(data.loc[data[data.columns[-1]]=="Yes"]) == len(data)):
            root.label = "Yes"
        else:
            root.label = "No"
        return root
    
    if(len(data.columns)==1):
        return
    else:
        attr = get_attr(data)
        root.label = attr
        values = set(data[attr])
        for value in values:
            root.branch[value] = decision_tree(data.loc[data[attr]==value].drop(attr,axis=1))
        return root

def entropy(data):
    total_ex = len(data)
    p_ex = len(data.loc[data['answer']=='Yes'])
    n_ex = len(data.loc[data['answer']=='No'])
    en = 0
    if(p_ex>0):
        en = -(p_ex/float(total_ex)) * (math.log(p_ex,2)-math.log(total_ex,2))
    if(n_ex>0):
        en += -(n_ex/float(total_ex)) * (math.log(n_ex,2)-math.log(total_ex,2))
        
    return en

def get_attr(data):
    en_s = entropy(data)
    attribute = ""
    max_gain = 0
    for attr in data.columns[:len(data.columns)-1]:
        g = gain(en_s, data, attr)
        if g > max_gain:
            max_gain = g
            attribute = attr
    
    return attribute

def gain(en_s,data_s,attrib):
    values = set(data_s[attrib])
    #print(values)
    gain = en_s
    for value in values:
        gain -= len(data_s.loc[data_s[attrib]==value])/float(len(data_s)) * entropy(data_s.loc[data_s[attrib]==value])
    
    return gain

def get_rules(root, rule, rules):
    if not root.branch:
        rules.append(rule[:-1]+"=>"+root.label)
        return rules
    for val in root.branch:
        get_rules(root.branch[val], rule+root.label+"="+str(val)+"^", rules)
    return rules

def test(tree, test_str):
    if not tree.branch:
        return tree.label
    return test(tree.branch[str(test_str[tree.label])], test_str)

data = pd.read_csv("playtennis.csv")
tree = decision_tree(data)
rules = get_rules(tree," ",[])
for rule in rules:
    print(rule)
test_str = {}
print("Enter the test case input: ")
for attr in data.columns[:-1]:
    test_str[attr] = input(attr+": ")
print(test_str)
print(test(tree, test_str))
